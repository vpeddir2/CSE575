{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b251bd58",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496b83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "import itertools\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/imdb-dataset.csv.gz'\n",
    "POSITIVE_LABEL = 'positive'\n",
    "NEGATIVE_LABEL = 'negative'\n",
    "VALID_CHARS = string.ascii_lowercase + string.digits + ' '\n",
    "INVALID_CHARS = set(string.printable).difference(VALID_CHARS)\n",
    "LOWERCASE_TRANSLATOR = str.maketrans({c: '' for c in INVALID_CHARS})\n",
    "NUM_EMBEDDING_DIMENSIONS = 50\n",
    "NUM_HIDDEN_DIMENSIONS = 128\n",
    "NUM_OUTPUT_DIMENSIONS = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "USE_KEYED_FLAG = 'k'\n",
    "USE_SENTIMENT_FLAG = 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92660cea",
   "metadata": {},
   "source": [
    "# Dataset & Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4feeade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = [torch.tensor(np.array(review[0]), dtype=torch.long) for review in reviews]\n",
    "        self.labels = torch.tensor([review[1] for review in reviews], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d300b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-s', '--space', choices=[USE_KEYED_FLAG, USE_SENTIMENT_FLAG], default=USE_KEYED_FLAG)\n",
    "    parser.add_argument('-n', '--num_of_records', type=int, default=-1,\n",
    "                        help=\"The number of records to read from file. Defaults to all.\")\n",
    "    args = parser.parse_args()\n",
    "    return args.space, args.num_of_records\n",
    "\n",
    "\n",
    "def yield_all_data_lines():\n",
    "    with gzip.open(DATA_PATH, mode='rt', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line\n",
    "\n",
    "\n",
    "def read_csv(lines, num_of_records):\n",
    "    reader = csv.reader(line for line in lines)\n",
    "    if num_of_records > 0:\n",
    "        return itertools.islice((row for row in reader), num_of_records)\n",
    "    else:\n",
    "        return (row for row in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4a346",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2153783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_labels(data):\n",
    "    for i, d in enumerate(data):\n",
    "        if i > 0:\n",
    "            if d[1] != POSITIVE_LABEL and d[1] != NEGATIVE_LABEL:\n",
    "                raise Exception(d[1])\n",
    "            yield d\n",
    "\n",
    "\n",
    "def labels_to_bools(data):\n",
    "    for d in data:\n",
    "        d[1] = 1 if d[1] == POSITIVE_LABEL else 0\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_lowercase(data):\n",
    "    for d in data:\n",
    "        d[0] = d[0].lower()\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_alphanumeric_words(data):\n",
    "    for d in data:\n",
    "        d[0] = d[0].translate(LOWERCASE_TRANSLATOR)\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_alphanum_chars(text):\n",
    "    return ''.join([c for c in text if c in VALID_CHARS])\n",
    "\n",
    "\n",
    "def remove_html_tags(data):\n",
    "    for d in data:\n",
    "        pattern = re.compile('<.*?>')\n",
    "        d[0] = pattern.sub(r'', d[0])\n",
    "        yield d\n",
    "\n",
    "def remove_url(data):\n",
    "    for d in data:\n",
    "        pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        d[0]=pattern.sub(r'', d[0])\n",
    "        yield d\n",
    "\n",
    "def remove_punc(data):\n",
    "    for d in data:\n",
    "        exclude =  string.punctuation\n",
    "        for ch in exclude:\n",
    "            d[0] = d[0].replace(ch,'')\n",
    "        yield d\n",
    "\n",
    "def chat_word_conversion(data):\n",
    "    url = \"https://raw.githubusercontent.com/MFuchs1989/Datasets-and-Miscellaneous/main/datasets/NLP/Text%20Pre-Processing%20VII%20(Special%20Cases)/chat_expressions.csv\" \n",
    "    chat_expressions = pd.read_csv(url, error_bad_lines=False)\n",
    "    chat_words = dict(zip(chat_expressions.Chat_Words, chat_expressions.Chat_Words_Extended))\n",
    "    for d in data:\n",
    "        new_text = []\n",
    "        for w in d[0].split():\n",
    "            if w.upper() in chat_words:\n",
    "                new_text.append(chat_words[w.upper()])\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        d[0] = \" \".join(new_text)        \n",
    "        yield d \n",
    "\n",
    "def correct_text(data):\n",
    "    for d in data:\n",
    "        textBlb = TextBlob(d[0])\n",
    "        d[0]=textBlb.correct().string\n",
    "        yield d \n",
    "\n",
    "def stop_words_removal(data):\n",
    "    for d in data:\n",
    "        new_text = []\n",
    "        for word in d[0].split():\n",
    "             if word in stopwords.words('english'):\n",
    "                new_text.append('')\n",
    "             else:\n",
    "                new_text.append(word)\n",
    "        x = new_text[:]\n",
    "        new_text.clear()\n",
    "        d[0] = \" \".join(x)\n",
    "        yield d\n",
    "\n",
    "def replace_emoji(data):\n",
    "    for d in data:\n",
    "        d[0]=emoji.demojize(d[0])\n",
    "        yield d \n",
    "\n",
    "def to_word_tuples(data):\n",
    "    for d in data:\n",
    "        d[0] = tuple(d[0].split(' '))\n",
    "        yield d\n",
    "\n",
    "def compose(fns, data):\n",
    "    if len(fns) == 1:\n",
    "        return fns[0](data)\n",
    "    return compose(fns[1:], fns[0](data))\n",
    "\n",
    "def preprocess_data(data):\n",
    "    processors = [\n",
    "        verify_labels,\n",
    "        labels_to_bools,\n",
    "        to_lowercase,\n",
    "        to_alphanumeric_words,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        remove_punc,\n",
    "        chat_word_conversion,\n",
    "        correct_text,\n",
    "        stop_words_removal,\n",
    "        replace_emoji,\n",
    "        to_word_tuples,\n",
    "    ]\n",
    "    return tuple(row for row in compose(processors, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b389d",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c210dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(all_data):\n",
    "    cutoff = len(all_data) // 2\n",
    "    return [all_data[:cutoff], all_data[cutoff:]]\n",
    "\n",
    "def to_sentiment_score_vectors(labeled_reviews):\n",
    "    counts_by_word = count_word_frequencies(labeled_reviews)\n",
    "    words_to_key = {word: (counts[0] - counts[1] / (counts[0] + counts[1])) for word, counts in counts_by_word.items()}\n",
    "    vectorized_data = tuple((tuple(words_to_key[word] for word in review[0]), review[1]) for review in labeled_reviews)\n",
    "    training_data, test_data = split_data(vectorized_data)\n",
    "    return training_data, test_data, len(counts_by_word)\n",
    "\n",
    "\n",
    "def count_word_frequencies(labeled_reviews):\n",
    "    result = {}\n",
    "    for review in labeled_reviews:\n",
    "        for word in review[0]:\n",
    "            if word not in result:\n",
    "                result[word] = np.array([0, 0], dtype=np.int32)\n",
    "            if review[1]:\n",
    "                result[word][0] += 1\n",
    "            else:\n",
    "                result[word][1] += 1\n",
    "    return result\n",
    "\n",
    "def to_key_number_vectors(labeled_reviews):\n",
    "    all_words = set(word for review in labeled_reviews for word in review[0])\n",
    "    words_to_key = {word: i + 1 for i, word in enumerate(all_words)}\n",
    "    vectorized_data = tuple((tuple(words_to_key[word] for word in review[0]), review[1]) for review in labeled_reviews)\n",
    "    training_data, test_data = split_data(vectorized_data)\n",
    "    return training_data, test_data, len(all_words)\n",
    "\n",
    "def choose_space(space_type):\n",
    "    if space_type is USE_SENTIMENT_FLAG:\n",
    "        print('Using sentiment-based space')\n",
    "        return to_sentiment_score_vectors\n",
    "    else:\n",
    "        print('Using key-based space')\n",
    "        return to_key_number_vectors\n",
    "    \n",
    "def vectorize(labeled_reviews, to_vectors):\n",
    "    return to_vectors(labeled_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22709b",
   "metadata": {},
   "source": [
    "# Class LSTM Architecture & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c93ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        return self.output_layer(hidden[-1])\n",
    "\n",
    "\n",
    "class SentimentClassifier:\n",
    "\n",
    "    def __init__(self, train_data, test_data, vocab_size, padding_element=0.0):\n",
    "        self.padding_element = padding_element\n",
    "        self.train_dataset = MovieReviewsDataset(train_data)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                       collate_fn=self.collate_fn)\n",
    "        self.test_dataset = MovieReviewsDataset(test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                      collate_fn=self.collate_fn)\n",
    "        self.lstm = SentimentLSTM(vocab_size, embedding_dim=NUM_EMBEDDING_DIMENSIONS, hidden_dim=NUM_HIDDEN_DIMENSIONS,\n",
    "                                  output_dim=NUM_OUTPUT_DIMENSIONS)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.lstm.parameters())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        reviews, labels = zip(*batch)\n",
    "        reviews_padded = pad_sequence(reviews, batch_first=True, padding_value=0.0)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return reviews_padded, labels\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print('Starting epoch ' + str(epoch + 1))\n",
    "            start = time.perf_counter()\n",
    "            self.train()\n",
    "            avg_val_loss, avg_val_accuracy = self.evaluate()\n",
    "            print(f\"Epoch: {epoch + 1}, Validation loss: {avg_val_loss}, Validation accuracy: {avg_val_accuracy}\",\n",
    "                  str(time.perf_counter() - start) + 's')\n",
    "\n",
    "    def train(self):\n",
    "        self.lstm.train()\n",
    "        for inputs, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.lstm(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.lstm.eval()\n",
    "        total_eval_loss = 0\n",
    "        total_eval_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                outputs = self.lstm(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                total_eval_loss += loss.item()\n",
    "                preds = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "                total_eval_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(self.test_loader)\n",
    "        avg_val_accuracy = total_eval_accuracy / len(self.test_dataset)\n",
    "        return avg_val_loss, avg_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc257298",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8d7176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-s {k,s}] [-n NUM_OF_RECORDS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\vpeddir2\\AppData\\Roaming\\jupyter\\runtime\\kernel-7a429bf9-a80b-49ec-825f-eb2070728e20.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vpeddir2\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    space_type, num_of_records = parse_args()\n",
    "    start = time.perf_counter()\n",
    "    train_data, test_data, size = vectorize(preprocess_data(read_csv(yield_all_data_lines(), num_of_records)),\n",
    "                                            choose_space(space_type))\n",
    "    print('vectorized data', time.perf_counter() - start)\n",
    "    classifier = SentimentClassifier(train_data, test_data, size)\n",
    "    classifier.train_and_evaluate()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5f5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
