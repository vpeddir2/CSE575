{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5316af",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e425471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vpeddir2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "import itertools\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import re\n",
    "import emoji\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a311696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/imdb-dataset.csv.gz'\n",
    "POSITIVE_LABEL = 'positive'\n",
    "NEGATIVE_LABEL = 'negative'\n",
    "VALID_CHARS = string.ascii_lowercase + string.digits + ' '\n",
    "INVALID_CHARS = set(string.printable).difference(VALID_CHARS)\n",
    "LOWERCASE_TRANSLATOR = str.maketrans({c: '' for c in INVALID_CHARS})\n",
    "NUM_EMBEDDING_DIMENSIONS = 50\n",
    "NUM_HIDDEN_DIMENSIONS = 128\n",
    "NUM_OUTPUT_DIMENSIONS = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "USE_KEYED_FLAG = 'k'\n",
    "USE_SENTIMENT_FLAG = 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2893c6",
   "metadata": {},
   "source": [
    "# Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eebe1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = [torch.tensor(np.array(review[0]), dtype=torch.long) for review in reviews]\n",
    "        self.labels = torch.tensor([review[1] for review in reviews], dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "894ff4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-s', '--space', choices=[USE_KEYED_FLAG, USE_SENTIMENT_FLAG], default=USE_KEYED_FLAG)\n",
    "    parser.add_argument('-n', '--num_of_records', type=int, default=-1,\n",
    "                        help=\"The number of records to read from file. Defaults to all.\")\n",
    "    args = parser.parse_args()\n",
    "    return args.space, args.num_of_records\n",
    "\n",
    "\n",
    "def yield_all_data_lines():\n",
    "    with gzip.open(DATA_PATH, mode='rt', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line\n",
    "\n",
    "\n",
    "def read_csv(lines, num_of_records):\n",
    "    reader = csv.reader(line for line in lines)\n",
    "    if num_of_records > 0:\n",
    "        return itertools.islice((row for row in reader), num_of_records)\n",
    "    else:\n",
    "        return (row for row in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8848e34",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ffc994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":OK_hand:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vpeddir2\\AppData\\Local\\Temp\\ipykernel_28776\\2479274458.py:51: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  chat_expressions = pd.read_csv(url, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "def verify_labels(data):\n",
    "    for i, d in enumerate(data):\n",
    "        if i > 0:\n",
    "            if d[1] != POSITIVE_LABEL and d[1] != NEGATIVE_LABEL:\n",
    "                raise Exception(d[1])\n",
    "            yield d\n",
    "\n",
    "\n",
    "def labels_to_bools(data):\n",
    "    for d in data:\n",
    "        d[1] = 1 if d[1] == POSITIVE_LABEL else 0\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_lowercase(data):\n",
    "    for d in data:\n",
    "        d[0] = d[0].lower()\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_alphanumeric_words(data):\n",
    "    for d in data:\n",
    "        d[0] = d[0].translate(LOWERCASE_TRANSLATOR)\n",
    "        yield d\n",
    "\n",
    "\n",
    "def to_alphanum_chars(text):\n",
    "    return ''.join([c for c in text if c in VALID_CHARS])\n",
    "\n",
    "\n",
    "def remove_html_tags(data):\n",
    "    for d in data:\n",
    "        pattern = re.compile('<.*?>')\n",
    "        d[0] = pattern.sub(r'', d[0])\n",
    "        yield d\n",
    "\n",
    "def remove_url(data):\n",
    "    for d in data:\n",
    "        pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        d[0]=pattern.sub(r'', d[0])\n",
    "        yield d\n",
    "\n",
    "def remove_punc(data):\n",
    "    for d in data:\n",
    "        exclude =  string.punctuation\n",
    "        for ch in exclude:\n",
    "            d[0] = d[0].replace(ch,'')\n",
    "        yield d\n",
    "        \n",
    "url = \"https://raw.githubusercontent.com/MFuchs1989/Datasets-and-Miscellaneous/main/datasets/NLP/Text%20Pre-Processing%20VII%20(Special%20Cases)/chat_expressions.csv\" \n",
    "chat_expressions = pd.read_csv(url, error_bad_lines=False)\n",
    "chat_words = dict(zip(chat_expressions.Chat_Words, chat_expressions.Chat_Words_Extended))\n",
    "\n",
    "def chat_word_conversion(data):\n",
    "    for d in data:\n",
    "        new_text = []\n",
    "        for w in d[0].split():\n",
    "            if w.upper() in chat_words:\n",
    "                new_text.append(chat_words[w.upper()])\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        d[0] = \" \".join(new_text)        \n",
    "        yield d \n",
    "\n",
    "def correct_text(data):\n",
    "    for d in data:\n",
    "        textBlb = TextBlob(d[0])\n",
    "        d[0]=textBlb.correct().string\n",
    "        yield d\n",
    "\n",
    "def stop_words_removal(data):\n",
    "    for d in data:\n",
    "        new_text = []\n",
    "        for word in d[0].split():\n",
    "             if word in stopwords.words('english'):\n",
    "                new_text.append('')\n",
    "             else:\n",
    "                new_text.append(word)\n",
    "        x = new_text[:]\n",
    "        new_text.clear()\n",
    "        d[0] = \" \".join(x)\n",
    "        yield d\n",
    "\n",
    "def replace_emoji(data):\n",
    "    for d in data:\n",
    "        d[0]=emoji.demojize(d[0])\n",
    "        yield d \n",
    "\n",
    "def to_word_tuples(data):\n",
    "    for d in data:\n",
    "        d[0] = tuple(d[0].split(' '))\n",
    "        yield d\n",
    "\n",
    "def compose(fns, data):\n",
    "    if len(fns) == 1:\n",
    "        return fns[0](data)\n",
    "    return compose(fns[1:], fns[0](data))\n",
    "\n",
    "def preprocess_data(data):\n",
    "    processors = [\n",
    "        verify_labels,\n",
    "        labels_to_bools,\n",
    "        to_lowercase,\n",
    "        to_alphanumeric_words,\n",
    "        remove_html_tags,\n",
    "        remove_url,\n",
    "        remove_punc,\n",
    "        chat_word_conversion,\n",
    "#         correct_text,\n",
    "#         stop_words_removal,\n",
    "        replace_emoji,\n",
    "        to_word_tuples,\n",
    "    ]\n",
    "    return tuple(row for row in compose(processors, data))\n",
    "\n",
    "print(emoji.demojize(\"ðŸ‘Œ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dd547",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8bd8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(all_data):\n",
    "    cutoff = len(all_data) // 2\n",
    "    return [all_data[:cutoff], all_data[cutoff:]]\n",
    "\n",
    "def to_sentiment_score_vectors(labeled_reviews):\n",
    "    counts_by_word = count_word_frequencies(labeled_reviews)\n",
    "    words_to_key = {word: (counts[0] - counts[1] / (counts[0] + counts[1])) for word, counts in counts_by_word.items()}\n",
    "    vectorized_data = tuple((tuple(words_to_key[word] for word in review[0]), review[1]) for review in labeled_reviews)\n",
    "    training_data, test_data = split_data(vectorized_data)\n",
    "    return training_data, test_data, len(counts_by_word)\n",
    "\n",
    "\n",
    "def count_word_frequencies(labeled_reviews):\n",
    "    result = {}\n",
    "    for review in labeled_reviews:\n",
    "        for word in review[0]:\n",
    "            if word not in result:\n",
    "                result[word] = np.array([0, 0], dtype=np.int32)\n",
    "            if review[1]:\n",
    "                result[word][0] += 1\n",
    "            else:\n",
    "                result[word][1] += 1\n",
    "    return result\n",
    "\n",
    "def to_key_number_vectors(labeled_reviews):\n",
    "    all_words = set(word for review in labeled_reviews for word in review[0])\n",
    "    words_to_key = {word: i + 1 for i, word in enumerate(all_words)}\n",
    "    vectorized_data = tuple((tuple(words_to_key[word] for word in review[0]), review[1]) for review in labeled_reviews)\n",
    "    training_data, test_data = split_data(vectorized_data)\n",
    "    return training_data, test_data, len(all_words)\n",
    "\n",
    "def choose_space(space_type):\n",
    "    if space_type is USE_SENTIMENT_FLAG:\n",
    "        print('Using sentiment-based space')\n",
    "        return to_sentiment_score_vectors\n",
    "    else:\n",
    "        print('Using key-based space')\n",
    "        return to_key_number_vectors\n",
    "    \n",
    "def vectorize(labeled_reviews, to_vectors):\n",
    "    return to_vectors(labeled_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e2efa",
   "metadata": {},
   "source": [
    "# Class LSTM Architecture & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3dbe7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size + 1, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        return self.output_layer(hidden[-1])\n",
    "\n",
    "\n",
    "class SentimentClassifier:\n",
    "\n",
    "    def __init__(self, train_data, test_data, vocab_size, padding_element=0.0):\n",
    "        self.padding_element = padding_element\n",
    "        self.train_dataset = MovieReviewsDataset(train_data)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                       collate_fn=self.collate_fn)\n",
    "        self.test_dataset = MovieReviewsDataset(test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                      collate_fn=self.collate_fn)\n",
    "        self.lstm = SentimentLSTM(vocab_size, embedding_dim=NUM_EMBEDDING_DIMENSIONS, hidden_dim=NUM_HIDDEN_DIMENSIONS,\n",
    "                                  output_dim=NUM_OUTPUT_DIMENSIONS)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.lstm.parameters())\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        reviews, labels = zip(*batch)\n",
    "        reviews_padded = pad_sequence(reviews, batch_first=True, padding_value=0.0)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return reviews_padded, labels\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print('Starting epoch ' + str(epoch + 1))\n",
    "            start = time.perf_counter()\n",
    "            self.train()\n",
    "            avg_val_loss, avg_val_accuracy = self.evaluate()\n",
    "            print(f\"Epoch: {epoch + 1}, Validation loss: {avg_val_loss}, Validation accuracy: {avg_val_accuracy}\",\n",
    "                  str(time.perf_counter() - start) + 's')\n",
    "\n",
    "    def train(self):\n",
    "        self.lstm.train()\n",
    "        for inputs, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.lstm(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.lstm.eval()\n",
    "        total_eval_loss = 0\n",
    "        total_eval_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                outputs = self.lstm(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                total_eval_loss += loss.item()\n",
    "                preds = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "                total_eval_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(self.test_loader)\n",
    "        avg_val_accuracy = total_eval_accuracy / len(self.test_dataset)\n",
    "        return avg_val_loss, avg_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1e626",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ce9c4ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mtrain_and_evaluate()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m space_type, num_of_records \u001b[38;5;241m=\u001b[39m USE_KEYED_FLAG, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m----> 4\u001b[0m train_data, test_data, size \u001b[38;5;241m=\u001b[39m vectorize(\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_all_data_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_records\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      5\u001b[0m                                         choose_space(space_type))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorized data\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m      7\u001b[0m classifier \u001b[38;5;241m=\u001b[39m SentimentClassifier(train_data, test_data, size)\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data):\n\u001b[0;32m    100\u001b[0m     processors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    101\u001b[0m         verify_labels,\n\u001b[0;32m    102\u001b[0m         labels_to_bools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m         to_word_tuples,\n\u001b[0;32m    113\u001b[0m     ]\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data):\n\u001b[0;32m    100\u001b[0m     processors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    101\u001b[0m         verify_labels,\n\u001b[0;32m    102\u001b[0m         labels_to_bools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m         to_word_tuples,\n\u001b[0;32m    113\u001b[0m     ]\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m compose(processors, data))\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mto_word_tuples\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_word_tuples\u001b[39m(data):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     91\u001b[0m         d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m d\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mreplace_emoji\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_emoji\u001b[39m(data):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     86\u001b[0m         d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m=\u001b[39memoji\u001b[38;5;241m.\u001b[39mdemojize(d[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m d\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mstop_words_removal\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_words_removal\u001b[39m(data):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     73\u001b[0m         new_text \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit():\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mcorrect_text\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     67\u001b[0m     textBlb \u001b[38;5;241m=\u001b[39m TextBlob(d[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 68\u001b[0m     d[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mtextBlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstring\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m d\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py:609\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m--> 609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py:608\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[0;32m    607\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 608\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m    609\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;124;03m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124;03m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\en\\__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py:1398\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w]) \\\n\u001b[1;32m-> 1398\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_known\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m   1399\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit2(w)) \\\n\u001b[0;32m   1400\u001b[0m           \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[0;32m   1401\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[0;32m   1402\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py:1381\u001b[0m, in \u001b[0;36mSpelling._known\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_known\u001b[39m(\u001b[38;5;28mself\u001b[39m, words\u001b[38;5;241m=\u001b[39m[]):\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the given list of words filtered by known words.\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py:1381\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_known\u001b[39m(\u001b[38;5;28mself\u001b[39m, words\u001b[38;5;241m=\u001b[39m[]):\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the given list of words filtered by known words.\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py:96\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__contains__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\textblob\\_text.py:87\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, types\u001b[38;5;241m.\u001b[39mMethodType(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mdict\u001b[39m, method), \u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mdict\u001b[39m, method)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    space_type, num_of_records = USE_KEYED_FLAG, -1\n",
    "    start = time.perf_counter()\n",
    "    train_data, test_data, size = vectorize(preprocess_data(read_csv(yield_all_data_lines(), num_of_records)),\n",
    "                                            choose_space(space_type))\n",
    "    print('vectorized data', time.perf_counter() - start)\n",
    "    classifier = SentimentClassifier(train_data, test_data, size)\n",
    "    classifier.train_and_evaluate()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading (remove generators), verify textblob first\n",
    "# Visualization of data (Word Clouds, N-grams, WordCount trend)\n",
    "# Preprocessing techniques (Print Headings,use TQDM, example at each)\n",
    "    # to Lowercase\n",
    "    # to Alphanumeric\n",
    "    # remove html tags, replace with space\n",
    "    # remove urls\n",
    "    # demojify\n",
    "    # remove punctuations\n",
    "    # remove blankspaces\n",
    "    # Spelling correction(textBlob, distilbert)\n",
    "    # remove stopwords\n",
    "    # lemmatization\n",
    "# 3 vectorizers->save processed data\n",
    "    # BagOfWords, CountVectorizer\n",
    "    # TFIDF\n",
    "    # DistilBert\n",
    "# model code in seperate cells, freeze and save models\n",
    "# run each model on 3 vectorizers\n",
    "# Evaluation Graphs, Summary Table\n",
    "# Demo test samples\n",
    "# Presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
